{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset, DatasetDict, load_dataset, Audio\n",
    "from peft import get_peft_model, LoraConfig, TaskType \n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchaudio.transforms import Resample\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face Hub\n",
    "dataset = load_dataset(\"irasalsabila/javanese_asr_dataset_20k\")\n",
    "\n",
    "# Access train and test splits\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = \"javanese_data\"\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "language = \"javanese\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper feature extractor, tokenizer, and processor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_id)\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "# model.to(device)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_id, language=language, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_id, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30 * 16000  # 30 seconds * 16,000 samples/second\n",
    "\n",
    "def load_audio(file_name):\n",
    "    file_path = os.path.join(audio_dir, file_name + \".flac\")\n",
    "\n",
    "    try:\n",
    "        speech, sr = torchaudio.load(file_path)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "            speech = resampler(speech)\n",
    "\n",
    "        speech = speech.squeeze(0)  # Remove extra dimensions\n",
    "\n",
    "        # Ensure consistent length (pad or truncate)\n",
    "        if speech.shape[0] > MAX_LENGTH:\n",
    "            speech = speech[:MAX_LENGTH]  # Truncate\n",
    "        else:\n",
    "            pad = MAX_LENGTH - speech.shape[0]\n",
    "            speech = torch.cat([speech, torch.zeros(pad)])  # Pad with zeros\n",
    "\n",
    "        return speech\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading {file_path}: {e}\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    speeches = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for filename, label in zip(batch[\"filename\"], batch[\"label\"]):\n",
    "        speech = load_audio(filename)  # Load audio\n",
    "        if speech is not None:\n",
    "            speeches.append(speech)\n",
    "            valid_labels.append(label)\n",
    "\n",
    "    if len(speeches) == 0:\n",
    "        return {}  \n",
    "\n",
    "    speeches = torch.stack(speeches).to(device)\n",
    "\n",
    "    inputs = feature_extractor(speeches.cpu().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    labels = tokenizer(valid_labels, return_tensors=\"pt\", padding=True).input_ids\n",
    "    labels = [l.tolist() for l in labels] \n",
    "\n",
    "    return {\n",
    "        \"input_features\": inputs.input_features.squeeze(0).to(device),  \n",
    "        \"labels\": labels  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(prepare_dataset, num_proc=1, batched=True, batch_size=16)\n",
    "test_dataset = test_dataset.map(prepare_dataset, num_proc=1, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'userid', 'label', 'input_features', 'labels'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (31/31 shards): 100%|██████████| 16000/16000 [04:52<00:00, 54.69 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|██████████| 4000/4000 [01:11<00:00, 56.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# # Save the processed dataset\n",
    "train_dataset.save_to_disk(\"processed_data/train_dataset\")\n",
    "test_dataset.save_to_disk(\"processed_data/test_dataset\")\n",
    "\n",
    "# # from datasets import load_from_disk\n",
    "# # train_dataset = load_from_disk(\"processed_data/train_dataset\")\n",
    "# # test_dataset = load_from_disk(\"processed_data/test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Handle audio input padding\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Handle text input padding\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding tokens with -100 for loss masking\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# Load WER evaluation metric\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Word Error Rate (WER)\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids if hasattr(pred, \"label_ids\") else None\n",
    "\n",
    "    # Replace -100 with the pad token ID\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER score\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  \n",
    ")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id, \\\n",
    "    quantization_config=quantization_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 36,864 || all params: 37,797,504 || trainable%: 0.0975\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=2, lora_alpha=4, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"temp\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=4,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=250,\n",
    "    eval_steps=2000,\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm-salsabila.pranida-62748/ipykernel_3395261/4179940389.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 1:17:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.054400</td>\n",
       "      <td>1.061775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.027400</td>\n",
       "      <td>1.009856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.992899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/salsabila.pranida/.conda/envs/nlp2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=1.0966273956298829, metrics={'train_runtime': 4654.1162, 'train_samples_per_second': 10.313, 'train_steps_per_second': 1.289, 'total_flos': 1.18425452544e+18, 'train_loss': 1.0966273956298829, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModel(\n",
      "      (base_model): LoraModel(\n",
      "        (model): PeftModel(\n",
      "          (base_model): LoraModel(\n",
      "            (model): WhisperForConditionalGeneration(\n",
      "              (model): WhisperModel(\n",
      "                (encoder): WhisperEncoder(\n",
      "                  (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                  (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "                  (embed_positions): Embedding(1500, 384)\n",
      "                  (layers): ModuleList(\n",
      "                    (0-3): 4 x WhisperEncoderLayer(\n",
      "                      (self_attn): WhisperSdpaAttention(\n",
      "                        (k_proj): Linear4bit(in_features=384, out_features=384, bias=False)\n",
      "                        (v_proj): lora.Linear4bit(\n",
      "                          (base_layer): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.05, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=384, out_features=2, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=2, out_features=384, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                          (lora_magnitude_vector): ModuleDict()\n",
      "                        )\n",
      "                        (q_proj): lora.Linear4bit(\n",
      "                          (base_layer): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.05, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=384, out_features=2, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=2, out_features=384, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                          (lora_magnitude_vector): ModuleDict()\n",
      "                        )\n",
      "                        (out_proj): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                      )\n",
      "                      (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                      (activation_fn): GELUActivation()\n",
      "                      (fc1): Linear4bit(in_features=384, out_features=1536, bias=True)\n",
      "                      (fc2): Linear4bit(in_features=1536, out_features=384, bias=True)\n",
      "                      (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                (decoder): WhisperDecoder(\n",
      "                  (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
      "                  (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
      "                  (layers): ModuleList(\n",
      "                    (0-3): 4 x WhisperDecoderLayer(\n",
      "                      (self_attn): WhisperSdpaAttention(\n",
      "                        (k_proj): Linear4bit(in_features=384, out_features=384, bias=False)\n",
      "                        (v_proj): lora.Linear4bit(\n",
      "                          (base_layer): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.05, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=384, out_features=2, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=2, out_features=384, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                          (lora_magnitude_vector): ModuleDict()\n",
      "                        )\n",
      "                        (q_proj): lora.Linear4bit(\n",
      "                          (base_layer): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.05, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=384, out_features=2, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=2, out_features=384, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                          (lora_magnitude_vector): ModuleDict()\n",
      "                        )\n",
      "                        (out_proj): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                      )\n",
      "                      (activation_fn): GELUActivation()\n",
      "                      (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                      (encoder_attn): WhisperSdpaAttention(\n",
      "                        (k_proj): Linear4bit(in_features=384, out_features=384, bias=False)\n",
      "                        (v_proj): lora.Linear4bit(\n",
      "                          (base_layer): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.05, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=384, out_features=2, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=2, out_features=384, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                          (lora_magnitude_vector): ModuleDict()\n",
      "                        )\n",
      "                        (q_proj): lora.Linear4bit(\n",
      "                          (base_layer): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.05, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=384, out_features=2, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=2, out_features=384, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                          (lora_magnitude_vector): ModuleDict()\n",
      "                        )\n",
      "                        (out_proj): Linear4bit(in_features=384, out_features=384, bias=True)\n",
      "                      )\n",
      "                      (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                      (fc1): Linear4bit(in_features=384, out_features=1536, bias=True)\n",
      "                      (fc2): Linear4bit(in_features=1536, out_features=384, bias=True)\n",
      "                      (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model ID: openai/whisper-tiny\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "print(model)  # ✅ Check if model is a PEFT model\n",
    "\n",
    "print(\"Model ID:\", model_id)  # ✅ Check model_id value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=2, target_modules={'q_proj', 'v_proj'}, lora_alpha=4, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))}\n"
     ]
    }
   ],
   "source": [
    "print(model.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORA\n"
     ]
    }
   ],
   "source": [
    "print(model.peft_config[\"default\"].peft_type.value)  # Should print \"LORA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-openai-whisper-tiny-LORA\n"
     ]
    }
   ],
   "source": [
    "if isinstance(model.peft_config, dict) and \"default\" in model.peft_config:\n",
    "    peft_type = model.peft_config[\"default\"].peft_type.value \n",
    "else:\n",
    "    peft_type = \"lora\" \n",
    "\n",
    "peft_model_id = f\"test/{model_id}-{peft_type}\".replace(\"/\", \"-\")\n",
    "print(peft_model_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA model saved to: test-openai-whisper-tiny-LORA\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(peft_model_id)\n",
    "print(f\"LoRA model saved to: {peft_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "\n",
    "peft_model_id = \"test-openai-whisper-tiny-LORA\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping={'base_model_class': 'PeftModel', 'parent_library': 'peft.peft_model'}, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=True, r=2, target_modules={'q_proj', 'v_proj'}, lora_alpha=4, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = WhisperForConditionalGeneration.from_pretrained(\n",
    "#     peft_config.base_model_name_or_path, quantization_config=quantization_config, \n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")  \n",
    "\n",
    "# model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 500/500 [10:40<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=128.9467055309655\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Ensure the model is on GPU\n",
    "model = model.to(\"cuda\")  # ✅ Move entire model to CUDA\n",
    "model_dtype = next(model.parameters()).dtype  # Get model dtype\n",
    "\n",
    "# Ensure LoRA layers are on GPU (sometimes required)\n",
    "if hasattr(model, \"base_model\"):\n",
    "    model.base_model.to(\"cuda\")\n",
    "\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.no_grad():  # No need for autocast since Whisper already handles mixed precision\n",
    "        # Ensure correct dtype for input_features\n",
    "        input_features = batch[\"input_features\"].to(\"cuda\").to(model_dtype)  # Convert to model dtype\n",
    "        decoder_input_ids = batch[\"labels\"][:, :4].to(\"cuda\")\n",
    "\n",
    "        generated_tokens = (\n",
    "            model.generate(\n",
    "                input_features=input_features,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                max_new_tokens=255,\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        metric.add_batch(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "        )\n",
    "\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "\n",
    "wer = 100 * metric.compute()\n",
    "print(f\"{wer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
